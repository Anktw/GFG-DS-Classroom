{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Lw-mqyunBCOjwYQhkNK9CYqPGA_VU3y2","timestamp":1762581676517}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introduction and Environment Setup\n","\n","The advent of powerful Large Language Models (LLMs) has fundamentally altered the landscape of artificial intelligence applications.\n","\n","The primary interface for these models is natural language, making the skill of crafting effective inputs—a discipline known as **prompt engineering**—a critical component for developers and researchers.\n","\n","Prompt engineering is not merely the act of asking a question; it is the science of designing and optimizing prompts to guide LLMs toward desired outcomes with precision, reliability, and efficiency.\n","\n","As models grow in capability, the sophistication of interaction methods must evolve in parallel.\n","\n","Simple instructions suffice for simple tasks, but unlocking the full potential for complex reasoning, persona adoption, and interaction with external systems requires a more advanced approach."],"metadata":{"id":"WkBtrAH4-uzB"}},{"cell_type":"markdown","source":["## Setup your free API Key using Google's AI Studio\n","\n","https://aistudio.google.com/\n"],"metadata":{"id":"LVdav--g_oQY"}},{"cell_type":"markdown","source":["### Install necessary libraries"],"metadata":{"id":"pYlXCB-H_yMj"}},{"cell_type":"markdown","source":["We will be installing of the google-generativeai package, the official Python SDK for the Gemini API."],"metadata":{"id":"zzdTvcbuABaP"}},{"cell_type":"code","source":["!pip3 install -q -U google-generativeai"],"metadata":{"id":"SLJSNTW-_t_P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Import required modules"],"metadata":{"id":"gBzmsfZt_2HV"}},{"cell_type":"code","source":["import os\n","import google.generativeai as genai\n","import textwrap\n","from IPython.display import display, Markdown\n","from google.colab import userdata"],"metadata":{"id":"RH3xCeNv_0ze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helper Functions"],"metadata":{"id":"M16xDMx7_6yx"}},{"cell_type":"code","source":["def to_markdown(text):\n","  text = text.replace('•', '  *')\n","  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"],"metadata":{"id":"a37ECeUj_5qq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def configure_gemini():\n","    \"\"\"\n","    Configures the Gemini API with an API key.\n","    Prioritizes the provided key, then environment variables, then user input.\n","    \"\"\"\n","    try:\n","        api_key = userdata.get('GOOGLE_API_KEY')\n","\n","        if not api_key:\n","            print(\"Error: GOOGLE_API_KEY environment variable not set.\")\n","            print(\"Please set the environment variable or paste your key when prompted.\")\n","            # Fallback to asking the user if the environment variable is not set.\n","            api_key = input(\"Please enter your Google API Key: \")\n","\n","        genai.configure(api_key=api_key)\n","        print(\"Gemini API configured successfully.\")\n","        return True\n","    except Exception as e:\n","        print(f\"An error occurred during Gemini configuration: {e}\")\n","        return False\n","\n","\n","if configure_gemini():\n","    # Initialize the Gemini model client\n","    model = genai.GenerativeModel('gemini-1.5-flash')\n","    print(\"Gemini model client initialized successfully.\")"],"metadata":{"id":"vO42g_owARjC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758356275455,"user_tz":-330,"elapsed":723,"user":{"displayName":"Sampurn Rattan","userId":"16873352445824035048"}},"outputId":"7a25437d-ce10-43e2-86ca-9e46ce4ee660"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gemini API configured successfully.\n","Gemini model client initialized successfully.\n"]}]},{"cell_type":"markdown","source":["# The Anatomy of an Effective Prompt: A Unified Framework"],"metadata":{"id":"ixGA2KiQARSH"}},{"cell_type":"markdown","source":["A fundamental advancement in prompt engineering is the realization that a prompt is not a monolithic question but a structured document composed of distinct components.\n","\n"],"metadata":{"id":"1nPVAszT_48D"}},{"cell_type":"markdown","source":["- **Role (or Persona):** This component defines who the model should be. Assigning a role, such as \"You are a senior technical support specialist,\" constrains the model's vast knowledge base, forcing it to filter its response through a specific lens of expertise, tone, and style. This dramatically improves the coherence and domain-specificity of the output.\n","\n","\n","- **Context (or Background Information):** This provides the necessary background for the task. It can include user history, product documentation, previous conversation turns, or any other data that informs the query. Providing rich context is essential for generating relevant and personalized responses.\n","\n","\n","- **Task (or Instruction/Directive):** This is the core of the prompt—a clear, specific, and unambiguous statement of the action the model should perform. The use of direct action verbs (e.g., \"Analyze,\" \"Summarize,\" \"Generate,\" \"Classify\") is critical for clarity.\n","\n","\n","- **Examples (or Shots):** These are high-quality examples of the desired input-output pattern. They are the foundation of few-shot prompting and are one of the most powerful tools for controlling output format and style. By showing the model exactly what is expected, examples enable a form of in-context learning.\n","\n","\n","- **Constraints (or Rules/Warnings):** This component defines the boundaries for the response. It specifies what the model should not do, such as avoiding certain topics, adhering to a word count, or refraining from using technical jargon. These \"guardrails\" are crucial for safety and brand alignment.\n","\n","\n","- **Output Format (or Structure):** This explicitly defines the structure of the desired output, such as JSON, Markdown, or a bulleted list. Specifying the format is vital for applications that need to programmatically parse the model's response, as it ensures the output is machine-readable and consistent."],"metadata":{"id":"7YrMcbznBGy1"}},{"cell_type":"markdown","source":["To help the model distinguish between these different components, it is a best practice to use clear delimiters. Structuring the prompt with markers like Markdown headers (e.g., `###Instruction###`)."],"metadata":{"id":"SX4PB6P9B2Vo"}},{"cell_type":"markdown","source":["# Foundational Prompting"],"metadata":{"id":"8Jy1LiY0COG-"}},{"cell_type":"markdown","source":["## Zero-Shot Prompting"],"metadata":{"id":"51NW4AedCPhj"}},{"cell_type":"markdown","source":["Zero-shot prompting is the most basic form of interaction with an LLM.\n"],"metadata":{"id":"0lE1IR85Cfae"}},{"cell_type":"code","source":["customer_email_1 = \"\"\"\n","        Hi team, my invoice for last month (INV-2024-05-AB) seems incorrect.\n","        The amount is higher than I expected. Can you please check it?\n","        \"\"\"\n","\n","# https://www.geeksforgeeks.org/python/formatted-string-literals-f-strings-python/\n","classification_prompt_zero_shot = f\"\"\"\n","      Classify the following customer email into one of the following categories:\n","      - Billing Inquiry\n","      - Technical Support\n","      - Feature Request\n","      - General Question\n","\n","      Email:\n","      \"{customer_email_1}\"\n","\n","      Classification:\n","\"\"\"\n","\n","response = model.generate_content(classification_prompt_zero_shot)\n","\n","# Display the output\n","to_markdown(response.text)"],"metadata":{"id":"mZviESF4CoE7","colab":{"base_uri":"https://localhost:8080/","height":82},"executionInfo":{"status":"ok","timestamp":1758356277857,"user_tz":-330,"elapsed":2400,"user":{"displayName":"Sampurn Rattan","userId":"16873352445824035048"}},"outputId":"2913e8eb-e05f-4baf-d340-f80aa26e534c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> Billing Inquiry\n"},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","source":["## Few-Shot Prompting"],"metadata":{"id":"uBVXNEYqCnuR"}},{"cell_type":"markdown","source":["Few-shot prompting is a powerful technique that leverages the model's in-context learning capabilities. By providing a small number of high-quality examples (or \"shots\") within the prompt, the developer can guide the model toward the desired output format, tone, and reasoning pattern."],"metadata":{"id":"F5NosurRFZ_m"}},{"cell_type":"code","source":["# Enhance the prompt with few-shot examples and a defined JSON output structure\n","\n","classification_prompt_few_shot = f\"\"\"\n","    Your task is to classify customer emails and extract key information.\n","    Format the output as a JSON object with the keys \"category\" and \"priority\".\n","\n","    ---\n","    <example>\n","    Input: \"Hello, I can't seem to reset my password. The link you sent is expired.\"\n","    Output:\n","    ```json\n","    {{\n","      \"category\": \"Technical Support\",\n","      \"priority\": \"High\"\n","    }}\n","    ```\n","\n","\n","    Input: \"{customer_email_1}\"\n","    Output:\n","\"\"\"\n","\n","\n","response = model.generate_content(classification_prompt_few_shot)\n","to_markdown(response.text)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":132},"id":"jry8oXDIBGNU","executionInfo":{"status":"ok","timestamp":1758356278967,"user_tz":-330,"elapsed":1108,"user":{"displayName":"Sampurn Rattan","userId":"16873352445824035048"}},"outputId":"1b35bf29-5912-487e-eb09-2800fa09cbaa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> ```json\n> {\n>   \"category\": \"Billing Inquiry\",\n>   \"priority\": \"Medium\"\n> }\n> ```\n"},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["# Adding Reason to your Prompts"],"metadata":{"id":"d6Qq3ALjF5Hy"}},{"cell_type":"markdown","source":["## Role-Based Prompting"],"metadata":{"id":"2QZc36iDGDu8"}},{"cell_type":"markdown","source":["Assigning a role or persona to an LLM is a simple yet profoundly effective technique for controlling its output. By starting a prompt with an instruction like \"You are a...\" the model is forced to filter its vast knowledge and generate a response from a specific perspective."],"metadata":{"id":"WVQgX5ROGGY-"}},{"cell_type":"code","source":["# The customer email from before\n","# customer_email_1 = \"Hi team, my invoice for last month (INV-2024-05-AB) seems incorrect. The amount is higher than I expected. Can you please check it?\"\n","\n","# Prompt A: No assigned role\n","prompt_no_role = f\"Draft a response to the following customer email:\\n\\n{customer_email_1}\"\n","\n","# Prompt B: With an assigned role\n","prompt_with_role = f\"\"\"\n","You are a patient and professional Senior Billing Specialist.\n","Your goal is to reassure the customer and resolve their issue efficiently.\n","\n","Draft a response to the following customer email:\n","{customer_email_1}\n","\"\"\"\n","\n","print(\"--- RESPONSE WITHOUT ROLE ---\")\n","response_a = model.generate_content(prompt_no_role)\n","display(to_markdown(response_a.text))\n","\n","print(\"- - \"*50)\n","print(\"- - \"*50)\n","\n","print(\"\\n\\n--- RESPONSE WITH ROLE ---\")\n","response_b = model.generate_content(prompt_with_role)\n","display(to_markdown(response_b.text))"],"metadata":{"id":"fqaYiTg2FsDJ","colab":{"base_uri":"https://localhost:8080/","height":672},"executionInfo":{"status":"ok","timestamp":1758356283570,"user_tz":-330,"elapsed":4601,"user":{"displayName":"Sampurn Rattan","userId":"16873352445824035048"}},"outputId":"bca7739b-c343-4717-91cf-d8daaa93aef2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- RESPONSE WITHOUT ROLE ---\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> Subject: Re: Invoice INV-2024-05-AB Inquiry\n> \n> Dear [Customer Name],\n> \n> Thank you for contacting us regarding invoice INV-2024-05-AB. We understand your concern about the invoice amount being higher than expected.\n> \n> We're happy to review your invoice. To assist us in investigating this efficiently, could you please provide us with some more details, such as:\n> \n> * **The amount you expected:**  This will help us quickly identify any discrepancies.\n> * **A brief description of your services/products used during the period:** This allows us to cross-reference the charges with your usage.\n> \n> Once we receive this information, we will thoroughly investigate the invoice and get back to you within [ timeframe, e.g., 24-48 hours] with an explanation and resolution.\n> \n> We appreciate your patience and understanding.\n> \n> Sincerely,\n> \n> The [Your Company Name] Team\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n","- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n","\n","\n","--- RESPONSE WITH ROLE ---\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> Subject: Re: Invoice INV-2024-05-AB Inquiry\n> \n> Dear [Customer Name],\n> \n> Thank you for contacting us regarding invoice INV-2024-05-AB.  I understand your concern about the invoice amount being higher than expected, and I'll be happy to assist you in reviewing it.\n> \n> To ensure a thorough investigation, could you please provide me with a brief description of what you believe is inaccurate on the invoice?  Knowing the specific discrepancy will allow me to expedite the review process.  For example, are you questioning a particular line item, the total amount, or something else?\n> \n> Once I receive this information, I will personally review your invoice against our records and get back to you within [Timeframe, e.g., 24-48 hours] with an explanation and resolution.\n> \n> We appreciate your patience and understanding.\n> \n> Sincerely,\n> \n> [Your Name]\n> Senior Billing Specialist\n> [Company Name]\n> [Contact Information]\n"},"metadata":{}}]},{"cell_type":"markdown","source":["## Chain of Thought (CoT) Prompting: Making the Model \"Think Out Loud\""],"metadata":{"id":"6t0H2xt0G6ve"}},{"cell_type":"markdown","source":["Chain of Thought (CoT) prompting is a technique designed to improve an LLM's performance on complex tasks that require multi-step reasoning, such as arithmetic or logical deduction. It works by instructing the model to break down the problem and \"think out loud,\" generating a series of intermediate reasoning steps before arriving at a final answer."],"metadata":{"id":"1VyOyAYTHBX9"}},{"cell_type":"code","source":["# A more complex customer email requiring multi-step reasoning\n","customer_email_2 = \"\"\"\n","Hi,\n","\n","I'm looking at my bill for June and it seems wrong.\n","I was charged the full $50 for the Pro plan, but I downgraded to the Basic plan (which is $20/month) on June 15th.\n","The month of June has 30 days.\n","Also, I have a 10% discount code (SAVE10) that should have been applied to the entire bill.\n","\n","Can you please calculate the correct amount I should have been charged?\n","\n","Thanks,\n","Alex\n","\"\"\"\n","\n","# Prompt A: Standard prompt, likely to fail\n","prompt_standard_reasoning = f\"\"\"\n","Based on the following email, calculate the correct final bill amount.\n","\n","Email:\n","{customer_email_2}\n","\n","Final Amount:\n","\"\"\"\n","\n","# Prompt B: Chain of Thought prompt\n","prompt_cot_reasoning = f\"\"\"\n","Based on the following email, calculate the correct final bill amount.\n","First, think step-by-step to break down the calculation. Explain your reasoning.\n","Then, provide the final answer in the format \"Final Amount: $XX.XX\".\n","\n","Email:\n","{customer_email_2}\n","\"\"\"\n","\n","print(\"--- STANDARD PROMPT (Likely Incorrect) ---\")\n","response_a = model.generate_content(prompt_standard_reasoning)\n","display(to_markdown(response_a.text))\n","\n","print(\"\\n\\n--- CHAIN OF THOUGHT PROMPT (Likely Correct) ---\")\n","response_b = model.generate_content(prompt_cot_reasoning)\n","display(to_markdown(response_b.text))"],"metadata":{"id":"RWbW8i10GaGi","colab":{"base_uri":"https://localhost:8080/","height":558},"executionInfo":{"status":"ok","timestamp":1758356288768,"user_tz":-330,"elapsed":5197,"user":{"displayName":"Sampurn Rattan","userId":"16873352445824035048"}},"outputId":"a1287cb1-68c9-4ba5-f37c-b3806f35c08f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- STANDARD PROMPT (Likely Incorrect) ---\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> Here's the breakdown of the correct bill amount:\n> \n> * **Pro Plan Days:** Alex used the Pro plan for the first 15 days of June (June 1st-15th).  Cost: (15/30) * $50 = $25\n> * **Basic Plan Days:** Alex used the Basic plan for the remaining 15 days of June (June 16th-30th). Cost: (15/30) * $20 = $10\n> * **Subtotal before discount:** $25 + $10 = $35\n> * **Discount:** $35 * 0.10 = $3.50\n> * **Final Amount:** $35 - $3.50 = $31.50\n> \n> Final Amount: $31.50\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","--- CHAIN OF THOUGHT PROMPT (Likely Correct) ---\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> Here's how to calculate the correct bill amount for Alex:\n> \n> **Step 1: Calculate the cost for the Pro plan.**\n> \n> * Alex used the Pro plan for the first 15 days of June.\n> * Daily cost of the Pro plan: $50 / 30 days = $1.67 per day (approximately)\n> * Cost for the Pro plan for the first 15 days: $1.67/day * 15 days = $25.05\n> \n> **Step 2: Calculate the cost for the Basic plan.**\n> \n> * Alex used the Basic plan for the remaining 15 days of June.\n> * Daily cost of the Basic plan: $20 / 30 days = $0.67 per day (approximately)\n> * Cost for the Basic plan for the last 15 days: $0.67/day * 15 days = $10.05\n> \n> **Step 3: Calculate the total cost before discount.**\n> \n> * Total cost before discount: $25.05 (Pro plan) + $10.05 (Basic plan) = $35.10\n> \n> **Step 4: Apply the discount.**\n> \n> * Discount amount: $35.10 * 0.10 = $3.51\n> * Total cost after discount: $35.10 - $3.51 = $31.59\n> \n> **Final Amount: $31.59**\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# Towards Agents"],"metadata":{"id":"_R5ZJEhyHYRV"}},{"cell_type":"markdown","source":["## ReAct Prompting"],"metadata":{"id":"LlH9VBi0Hr4j"}},{"cell_type":"markdown","source":["ReAct (**Re**asoning and **Act**ing) represents a paradigm shift in how LLMs can solve problems. It combines the internal reasoning of Chain of Thought with the ability to take Actions and process Observations from external tools or APIs. This creates a powerful feedback loop: the model **Thinks** about what it needs to know, performs an **Action** (like a search query or API call) to get that information, receives an **Observation** (the result of the action), and then uses that new information in its next step.\n","\n","The primary benefit of this approach is its ability to ground the model's reasoning in external, factual, and up-to-date information.\n","\n","This `Thought -> Action -> Observation` cycle is more than just a prompting technique; it is the fundamental foundation of an agentic AI."],"metadata":{"id":"NgiWRWQMH8Tz"}},{"cell_type":"code","source":["# Mock function to simulate an external API call\n","def lookup_order_status(order_id: str) -> str:\n","    \"\"\"Simulates looking up an order status from a database or API.\"\"\"\n","    print(f\"\")\n","    statuses = {\n","        \"ORD-12345\": \"Your order has been shipped and is expected to arrive on July 5th, 2025.\",\n","        \"ORD-67890\": \"Your order is currently being processed and has not yet shipped.\",\n","        \"ORD-11223\": \"We could not find an order with that ID. Please double-check the number.\"\n","    }\n","    return statuses.get(order_id, \"Invalid order ID.\")"],"metadata":{"id":"Ja4bYiXOHLpj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# A new customer email requiring external tool use\n","customer_email_3 = \"Hi, I'm writing to check on the status of my recent order, #ORD-12345. Can you let me know where it is?\"\n","\n","# The ReAct prompt needs to be manually processed in a loop for this demonstration.\n","# Step 1: Initial Thought\n","react_prompt_1 = f\"\"\"\n","    You have access to a tool: `lookup_order_status(order_id: str)`.\n","    Given the user's query, decide if you need to use the tool.\n","    If so, respond with the tool call. If not, respond to the user directly.\n","\n","    User Query: \"{customer_email_3}\"\n","    Thought: The user is asking for the status of order #ORD-12345. I need to use the `lookup_order_status` tool to get this information.\n","    Action: lookup_order_status(order_id='ORD-12345')\n","\"\"\"\n","print(\"--- PROMPT TO GENERATE ACTION ---\")\n","print(react_prompt_1)\n","\n","# Step 2: Execute the Action and get the Observation\n","# In a real agent, this would be automated. Here, we call the function manually.\n","observation = lookup_order_status(order_id='ORD-12345')\n","print(f\"\\nObservation: {observation}\")\n","\n","\n","# Step 3: Use the Observation to generate the final answer\n","react_prompt_2 = f\"\"\"\n","    You have performed an action and received an observation.\n","    Now, formulate the final response to the user.\n","\n","    User Query: \"{customer_email_3}\"\n","    Action Taken: lookup_order_status(order_id='ORD-12345')\n","    Observation: \"{observation}\"\n","\n","    Thought: The tool returned that the order has been shipped and provided a delivery date. I should now convey this information clearly and helpfully to the customer.\n","    Final Answer:\n","\"\"\"\n","\n","print(\"\\n\\n--- PROMPT TO GENERATE FINAL ANSWER ---\")\n","final_response = model.generate_content(react_prompt_2)\n","display(to_markdown(final_response.text))"],"metadata":{"id":"ILAVvCXZIhHe","colab":{"base_uri":"https://localhost:8080/","height":351},"executionInfo":{"status":"ok","timestamp":1758356289976,"user_tz":-330,"elapsed":1202,"user":{"displayName":"Sampurn Rattan","userId":"16873352445824035048"}},"outputId":"6ccf63f2-09e3-4b06-d44c-01fea2115ae8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- PROMPT TO GENERATE ACTION ---\n","\n","    You have access to a tool: `lookup_order_status(order_id: str)`.\n","    Given the user's query, decide if you need to use the tool.\n","    If so, respond with the tool call. If not, respond to the user directly.\n","\n","    User Query: \"Hi, I'm writing to check on the status of my recent order, #ORD-12345. Can you let me know where it is?\"\n","    Thought: The user is asking for the status of order #ORD-12345. I need to use the `lookup_order_status` tool to get this information.\n","    Action: lookup_order_status(order_id='ORD-12345')\n","\n","\n","\n","Observation: Your order has been shipped and is expected to arrive on July 5th, 2025.\n","\n","\n","--- PROMPT TO GENERATE FINAL ANSWER ---\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> Hi there!  Your order, #ORD-12345, has shipped and is expected to arrive on July 5th, 2025.\n"},"metadata":{}}]},{"cell_type":"markdown","source":["# Implementing Safety Settings"],"metadata":{"id":"FE6mkixCJEnB"}},{"cell_type":"markdown","source":["AI safety is a paramount concern in the deployment of any LLM-powered application. Guardrails are safety mechanisms designed to monitor, filter, and control LLM inputs and outputs to prevent harmful, biased, illegal, or off-topic responses."],"metadata":{"id":"repNxZLFJWim"}},{"cell_type":"markdown","source":["The Gemini API, for instance, has built-in, configurable safety settings that can block content across categories like hate speech, harassment, and dangerous content."],"metadata":{"id":"7PLTSVwnJbiZ"}},{"cell_type":"code","source":["from google.generativeai.types import HarmCategory, HarmBlockThreshold\n","\n","# A prompt designed to be blocked by safety filters\n","provocative_prompt = \"Write a mean and insulting email to a customer who complained.\"\n","\n","try:\n","    response = model.generate_content(provocative_prompt)\n","    display(to_markdown(response.text))\n","except Exception as e:\n","    print(f\"Response was blocked by default safety filters: {e}\")"],"metadata":{"id":"QhZFdZchIqgi","colab":{"base_uri":"https://localhost:8080/","height":116},"executionInfo":{"status":"ok","timestamp":1758356291401,"user_tz":-330,"elapsed":1424,"user":{"displayName":"Sampurn Rattan","userId":"16873352445824035048"}},"outputId":"6b767a07-14d8-46aa-b663-5ec0862110c7"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> I cannot fulfill this request.  Sending a mean and insulting email to a customer is unprofessional, unethical, and likely to damage the business's reputation.  It's crucial to handle customer complaints with respect and professionalism, even if the complaint seems unreasonable.  A negative response could escalate the situation and lead to further problems.\n"},"metadata":{}}]},{"cell_type":"markdown","source":["### Application-level guardrail"],"metadata":{"id":"JzZEgssYJw4R"}},{"cell_type":"markdown","source":["This implementation creates a simple, programmatic guardrail. It uses a preliminary, low-cost LLM call to classify an incoming query. If the query is off-topic, it returns a canned response without ever engaging the more complex and expensive response-generation logic."],"metadata":{"id":"jyaYfQ7sJwFB"}},{"cell_type":"code","source":["def topical_guardrail(email_text: str):\n","    \"\"\"\n","    A simple guardrail to check if an email is on-topic.\n","    Returns True if on-topic, False otherwise.\n","    \"\"\"\n","    guardrail_prompt = f\"\"\"\n","    Is the following email about a customer support issue (billing, technical, etc.)?\n","    Answer with only \"Yes\" or \"No\".\n","\n","    Email: \"{email_text}\"\n","    Answer:\n","    \"\"\"\n","    # Using a smaller, faster model for this simple classification is cost-effective\n","    guardrail_model = genai.GenerativeModel('gemini-1.5-flash-latest')\n","    response = guardrail_model.generate_content(guardrail_prompt)\n","\n","    if \"yes\" in response.text.lower():\n","        print(\"Email is on-topic.\")\n","        return True\n","    else:\n","        print(\"Email is off-topic.\")\n","        return False\n","\n","def process_customer_email(email_text: str):\n","    \"\"\"\n","    Processes a customer email, first checking it against the topical guardrail.\n","    \"\"\"\n","    if topical_guardrail(email_text):\n","        # If the guardrail passes, proceed with the main logic (e.g., few-shot classification)\n","        print(\"Proceeding with full response generation...\")\n","        response = model.generate_content(classification_prompt_few_shot.replace(customer_email_1, email_text))\n","        display(to_markdown(response.text))\n","    else:\n","        # If the guardrail fails, return a canned response\n","        canned_response = \"I am a customer support assistant and can only help with questions about our products and services. For other inquiries, please contact our general information line.\"\n","        print(\"Returning canned response.\")\n","        display(to_markdown(canned_response))\n","\n","\n","# --- Test Cases ---\n","print(\"--- Testing On-Topic Email ---\")\n","on_topic_email = \"My bill is wrong again!\"\n","process_customer_email(on_topic_email)\n","\n","print(\"\\n\\n--- Testing Off-Topic Email ---\")\n","off_topic_email = \"Can you tell me a funny joke about computers?\"\n","process_customer_email(off_topic_email)"],"metadata":{"id":"gTFGMxtNJeGf","colab":{"base_uri":"https://localhost:8080/","height":328},"executionInfo":{"status":"ok","timestamp":1758356294542,"user_tz":-330,"elapsed":3140,"user":{"displayName":"Sampurn Rattan","userId":"16873352445824035048"}},"outputId":"eb303659-a120-4ba4-93bb-0cc7fd21ba2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Testing On-Topic Email ---\n","Email is on-topic.\n","Proceeding with full response generation...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> ```json\n> {\n>   \"category\": \"Billing Inquiry\",\n>   \"priority\": \"High\"\n> }\n> ```\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","\n","--- Testing Off-Topic Email ---\n","Email is off-topic.\n","Returning canned response.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Markdown object>"],"text/markdown":"> I am a customer support assistant and can only help with questions about our products and services. For other inquiries, please contact our general information line."},"metadata":{}}]},{"cell_type":"markdown","source":["https://www.geeksforgeeks.org/data-science/a-unified-framework-for-an-effective-prompt/"],"metadata":{"id":"bBJBJcarI5Ei"}},{"cell_type":"markdown","source":["# Want to Learn More?\n","Come Learn with Me: https://www.geeksforgeeks.org/courses/generative-ai-training-program"],"metadata":{"id":"hL77vJkrMsE-"}},{"cell_type":"code","source":[],"metadata":{"id":"oPrX71ktNBnr"},"execution_count":null,"outputs":[]}]}